{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f917932f",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Syntax-based Analysis (Part 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd56ba00",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "Disclaimer: This notebook is work in progress; explanations of the Code2vec and ASTNN models are still to be refined."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0bb2454",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Code Embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d83963d",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "In order to ing on to somewhat more classical examples of syntax-based analysis of code, we consider a recent trend: allow machine learning models to make predictions on code, we need to convert the source code to a format that is suitable as input for ML models, i.e., numerical vectors, typically referred to as _embeddings_. A common approach is to create these embeddings from the syntax tree."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c60c1e6f",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "A well-known example of embeddings in a different domain is Word2vec: Word2vec is a two-layer neural net that processes text by “vectorizing” words. Its input is a text corpus and its output is a set of vectors: feature vectors that represent words in that corpus. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9ae4f45",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "The purpose and usefulness of Word2vec is to group the vectors of similar words together in vectorspace. That is, it detects similarities mathematically. Word2vec creates vectors that are distributed numerical representations of word features, features such as the context of individual words."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d89c2870",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "Given enough data, usage and contexts, Word2vec can make highly accurate guesses about a word’s meaning based on past appearances. Those guesses can be used to establish a word’s association with other words (e.g. “man” is to “boy” what “woman” is to “girl”), or cluster documents and classify them by topic. Those clusters can form the basis of search, sentiment analysis and recommendations in such diverse fields as scientific research, legal discovery, e-commerce and customer relationship management."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66aee8fe",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "The output of the Word2vec neural net is a vocabulary in which each item has a vector attached to it, which can be fed into a deep-learning net or simply queried to detect relationships between words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7710dd4a",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "import gensim.downloader\n",
    "glove_vectors = gensim.downloader.load('glove-twitter-25')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2eb87ec1",
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "glove_vectors.most_similar('twitter')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04dc3ea3",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Code2vec"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a9b5fc3",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "A fundamental paper introducing the general idea of code embeddings is Code2Vec:\n",
    "\n",
    "Alon, U., Zilberstein, M., Levy, O., & Yahav, E. (2019). code2vec: Learning distributed representations of code. Proceedings of the ACM on Programming Languages, 3(POPL), 1-29."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ab35341",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "We will look at how to embed an individual method using code2vec. For this, let's define a simple helper function that gives us an AST rooted at a method declaration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c4c50ab",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "code = \"\"\"\n",
    "public int sum(int a, int b) {\n",
    "   return a + b + 2;\n",
    "}\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "817da8cf",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "import javalang"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "356e85bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_method(code):\n",
    "    class_code = \"class Dummy {\\n\" + code + \"\\n}\";\n",
    "    tokens = javalang.tokenizer.tokenize(class_code)\n",
    "    parser = javalang.parser.Parser(tokens)\n",
    "    ast = parser.parse()\n",
    "    _, node = list(ast.filter(javalang.tree.MethodDeclaration))[0]\n",
    "    return node"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5a94576",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "tree = parse_method(code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "693303ff",
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e1d3a6d",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "from graphviz import Digraph\n",
    "def print_tree(tree):\n",
    "    unique_id = 1\n",
    "    dot = Digraph()\n",
    "    for path, node in tree:\n",
    "        dot.node(str(id(node)), str(type(node)))\n",
    "        \n",
    "        for child in node.children:\n",
    "            if isinstance(child, javalang.ast.Node):\n",
    "                dot.edge(str(id(node)), str(id(child)))\n",
    "            elif type(child) == str:\n",
    "                strid = str(unique_id)\n",
    "                unique_id = unique_id + 1\n",
    "                dot.node(strid, child)\n",
    "                dot.edge(str(id(node)), strid)\n",
    "            elif type(child) == list:\n",
    "                for lc in child:\n",
    "                    dot.edge(str(id(node)), str(id(lc)))\n",
    "                 \n",
    "    return dot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c35ad698",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "print_tree(tree)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "887ad650",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "Code2vec looks at the concept of a path context, which is a path between two tokens in the AST.\n",
    "\n",
    "We can easily retrieve a list of all terminals in the AST; for example we could traverse the tree and look for strings or sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5c8660d",
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "for path, node in tree:\n",
    "    for child in node.children:\n",
    "        if child:\n",
    "            if type(child) is str:\n",
    "                print(\"Terminal: \", child)\n",
    "            elif type(child) is set:\n",
    "                for x in child:\n",
    "                    print(\"Terminal \", x)\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0ca94c8",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "Let's put this into a function that gives us the terminals as well as the corresponding AST nodes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f99db500",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "def get_terminal_nodes(tree):\n",
    "    for path, node in tree:\n",
    "        for child in node.children:\n",
    "            if child:\n",
    "                if type(child) is str and child != \"Dummy\":\n",
    "                    yield(node, child)\n",
    "                elif type(child) is set:\n",
    "                    for x in child:\n",
    "                        yield(node, x)      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d520e745",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "[ terminal for _, terminal in list(get_terminal_nodes(tree))]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6b24d21",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "A path context is defined as the path between two terminals, so let's pick to two terminals."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9c833de",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "node1, terminal1 = list(get_terminal_nodes(tree))[-1]\n",
    "node2, terminal2 = list(get_terminal_nodes(tree))[-2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b59397e",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "terminal1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcde0dda",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "terminal2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2b40715",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "Let's first construct the path from a root node to a chosen terminal node."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e893f79",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "def get_path(tree, node):\n",
    "    if tree == node:\n",
    "        return [tree]\n",
    "    \n",
    "    if type(tree) == list:\n",
    "        for child in tree:\n",
    "            path = get_path(child, node)\n",
    "            if path:\n",
    "                return path  \n",
    "    \n",
    "    if not isinstance(tree, javalang.tree.Node):\n",
    "        return None\n",
    "    \n",
    "    for child in tree.children:\n",
    "        path = get_path(child, node)\n",
    "        if path:\n",
    "            return [tree] + path  \n",
    "    \n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80a59c6a",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "def print_path(path):\n",
    "    result = \"\"\n",
    "    for node in path:\n",
    "        if type(node) == str:\n",
    "            result += node\n",
    "        elif type(node) == list:\n",
    "            result += print_path(node)\n",
    "        else:\n",
    "            result += str(type(node))\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cf285a6",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "print_path(get_path(tree, node1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b3c3810",
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "print_path(get_path(tree, node2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2949ad0",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "A path context consists of the path up the AST from the first terminal node to the least common ancestor of both terminal nodes, and then down the AST again to the second terminal node."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc0551f2",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "def path_context(tree, node1, node2):\n",
    "    path1 = get_path(tree, node2)\n",
    "    path1.reverse()\n",
    "    for i in range(len(path1)):\n",
    "        node = path1[i]\n",
    "        path2 = get_path(node, node1)\n",
    "        if path2:\n",
    "            return (path1[:i], path2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "763e1aa7",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "def print_path_context(path_context):\n",
    "    down_path = []\n",
    "    up_path = []\n",
    "    for node in path_context[0]:\n",
    "        if type(node) == str:\n",
    "            up_path.append(node)\n",
    "        else:\n",
    "            up_path.append(node.__class__.__name__)\n",
    "    for node in path_context[1]:\n",
    "        if type(node) == str:\n",
    "            down_path.append(node)\n",
    "        else:\n",
    "            down_path.append(node.__class__.__name__)\n",
    "            \n",
    "    return \"↑\".join(up_path) + \"↑\" + \"↓\".join(down_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "491f0509",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "print_path_context(path_context(tree, node1, node2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c47aa92",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "print_path_context(path_context(tree, node2, node1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a7591bc",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "terminal1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31d216d8",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "terminal2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d8fd98d",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "To build the embeddings for a method, we next require the path context for every pair of terminal nodes in the AST."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c61df9c6",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "terminals = list(get_terminal_nodes(tree))\n",
    "paths = []\n",
    "for index1 in range(len(terminals)-1):\n",
    "    node1, terminal1 = terminals[index1]\n",
    "    for index2 in range(index1 + 1, len(terminals)):\n",
    "        node2, terminal2 = terminals[index2]\n",
    "        path = path_context(tree, node1, node2)\n",
    "        print(terminal1, \",\", print_path_context(path), \",\", terminal2)\n",
    "        paths.append(path)\n",
    "len(paths)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ff9d5db",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "Converting a function to a path context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75a873b7",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "method1 = \"\"\"\n",
    "public int sum(int a, int b) {\n",
    "   return a + b + 2;\n",
    "}\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dfab4df",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "method2 = \"\"\"\n",
    "public void printHello(String name) {\n",
    "   System.out.println(\"Hello \" + name +\"! \");\n",
    "}\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88e433a2",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "method3 = \"\"\"\n",
    "public boolean isTheAnswer(int x) {\n",
    "   if (x == 42) {\n",
    "     return true;\n",
    "   } else {\n",
    "     return false;\n",
    "   }\n",
    "}\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32ee9b5e",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "def get_method_name(tree):\n",
    "    for _, node in tree.filter(javalang.tree.MethodDeclaration):\n",
    "        return node.name\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8aa73c53",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "def get_id(value, dictionary, vocab):\n",
    "    if value in dictionary:\n",
    "        return dictionary[value]\n",
    "    else:\n",
    "        new_id = len(dictionary.keys())\n",
    "        dictionary[value] = new_id\n",
    "        vocab[new_id] = value\n",
    "    return new_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e93670de",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "\n",
    "# collection of all known paths, terminals, and method names in the dataset\n",
    "@dataclass\n",
    "class Vocabulary:\n",
    "    # actual type of the value is not important, put in whatever is best\n",
    "    paths: dict[int, str]\n",
    "    terminals: dict[int, str]\n",
    "    method_names: dict[int, str]\n",
    "\n",
    "vocabulary = Vocabulary({}, {}, {})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97b68c24",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "train_x = []\n",
    "train_y = []\n",
    "\n",
    "paths = {}\n",
    "method_names = {}\n",
    "terminal_names = {}\n",
    "\n",
    "for method in [method1, method2, method3]:\n",
    "    method_ast = parse_method(method)\n",
    "    name = get_method_name(method_ast)    \n",
    "    method_id = get_id(name, method_names, vocabulary.method_names)\n",
    "    path_contexts = []\n",
    "    \n",
    "    terminals = list(get_terminal_nodes(method_ast))\n",
    "    for index1 in range(len(terminals)-1):\n",
    "        node1, terminal1 = terminals[index1]\n",
    "        terminal1_id = get_id(terminal1, terminal_names, vocabulary.terminals)\n",
    "        for index2 in range(index1 + 1, len(terminals)):\n",
    "            node2, terminal2 = terminals[index2]\n",
    "            terminal2_id = get_id(terminal2, terminal_names, vocabulary.terminals)\n",
    "            path = path_context(method_ast, node1, node2)\n",
    "            path_str = print_path_context(path)\n",
    "            path_id = get_id(path_str, paths, vocabulary.paths)\n",
    "            print(terminal1, \",\", path_str, \",\", terminal2)\n",
    "            print(terminal1_id, \",\", path_id, \",\", terminal2_id)\n",
    "            path_contexts.append((terminal1_id, path_id, terminal2_id))\n",
    "            \n",
    "    train_x.append(path_contexts)\n",
    "    train_y.append(method_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d4ddb86",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "train_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e16c14b7",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "train_y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "612fcffa",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "### Embedding Path Contexts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adb0587e",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "With a method represented as a bag of path contexts, where each context is represented as a vector of learned values. Before being able to feed this into a neural network, we need to create a single vector for a method. Code2vec achieves this by creating a weighted average context vector, where the weights are based on a global attention vector that is learned using standard neural backpropagation.\n",
    "\n",
    "Since the aim of this course lies in the software analysis we won't go into much details on the machine learning side of things, but at this point I'd suggest that you read the following paper to get a great overview: [Code2vec Paper](https://urialon.cswp.cs.technion.ac.il/wp-content/uploads/sites/83/2018/12/code2vec-popl19.pdf).\n",
    "\n",
    "Also, make sure to try out code embeddings at the [Code2vec interactive demo](https://code2vec.org)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02a7c7c6",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "![Code2vec Model](figures/code2vec.png) (Taken from the Code2vec paper)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b7bc360",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "In the following you can find the relevant code to create the model, but I won't go into much depth to explain this in detail."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52dc2efb",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "### Representation\n",
    "Each path is represented as a vector $p$ with values that are not known initially. The terminals are each represented by a vector $t$ with unknown elements as well. By concatenating the three parts of a context its representation $c_i = [t_\\mathrm{start}, p, t_\\mathrm{end}]$ is created.\n",
    "To learn how the different parts of $c_i$ relate to each other, a weight matrix $W$ with learnable weights is introduced. The product $\\tilde{c}_i := \\mathrm{tanh}(W \\cdot c_i)$ is then called a *combined context vector* as it now no longer contains just the concatenation of the three separate parts.\n",
    "\n",
    "The whole code snippet/method body is again represented as a single vector $v$. As different contexts of this code snippet are not equally important, the network has to learn which ones actually are. To achieve this, an attention vector that contains a weight $\\alpha_i$ for each context is learned. The code vector $v$ can then be calculated as the weighted sum\n",
    "$$\n",
    "    v := \\sum_{i=1}^{n} \\alpha_i \\cdot \\tilde{c}_i\n",
    "$$\n",
    "\n",
    "\n",
    "Each method name is again represented as a vector $y$ with unknown values. The probability $q(y)$ that a code vector should be associated with this tag is calculated as $q(y) := \\mathrm{softmax}(v^T \\cdot y)$. By performing this calculation for all known tags the one with the highest probability to fit the code can be chosen.\n",
    "\n",
    "### Learned Elements\n",
    "- A vector $c$ as representation for each context as combination of representations $p$ for paths and $t$ for terminals.\n",
    "- A weight matrix $W$ that contains information how the three parts of a context are combined.\n",
    "- An attention weight $\\alpha$ which contains information which contexts in a method are important.\n",
    "- A vector $t$ as representation for each method name."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14a18d66",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.python.layers.base import Layer\n",
    "from keras import Input, activations, optimizers, losses\n",
    "import keras.backend as kb\n",
    "from keras.layers import Embedding, Concatenate, Dropout, TimeDistributed, Dense\n",
    "\n",
    "# how many paths does the biggest analysed function have\n",
    "MAX_PATHS = 50\n",
    "# length of the vectors that should represent paths and labels (same size for simplicity)\n",
    "EMBEDDING_SIZE = 100\n",
    "# embedding sizes of start, path, end added together\n",
    "CONTEXT_EMBEDDING_SIZE = 3 * EMBEDDING_SIZE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d731da5a",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Adapted from: https://github.com/tech-srl/code2vec/blob/master/keras_model.py\n",
    "def build_code2vec_model(vocab: Vocabulary):\n",
    "    path_start_token = Input((MAX_PATHS,), dtype=tf.int32)\n",
    "    path_input = Input((MAX_PATHS,), dtype=tf.int32)\n",
    "    path_end_token = Input((MAX_PATHS,), dtype=tf.int32)\n",
    "    # the sets of contexts for each function are padded to contain MAX_PATHS number of paths\n",
    "    context_mask = Input((MAX_PATHS,))\n",
    "\n",
    "    # The elements of the matrix are chosen randomly, as the actual values have to be learned.\n",
    "    paths_embedded = Embedding(len(vocab.paths), EMBEDDING_SIZE,\n",
    "                               name='path_embedding')(path_input)\n",
    "\n",
    "    # Embed terminals the same way as paths.\n",
    "    token_embedding = Embedding(len(vocab.terminals), EMBEDDING_SIZE,\n",
    "                                name='token_embedding')\n",
    "    path_start_token_embedded = token_embedding(path_start_token)\n",
    "    path_end_token_embedded = token_embedding(path_end_token)\n",
    "\n",
    "    # Representation of contexts $c_i$: concatenation of start, path, end\n",
    "    context_embedded = Concatenate()([path_start_token_embedded, paths_embedded, path_end_token_embedded])\n",
    "    # Dropout to prevent overfitting.\n",
    "    context_embedded = Dropout(0.25)(context_embedded)\n",
    "\n",
    "    # $\\tilde{c}_i = tanh(Wc_i)$\n",
    "    # Fully connected layer that learns to combine the three parts of a context.\n",
    "    context_after_dense = TimeDistributed(\n",
    "        Dense(CONTEXT_EMBEDDING_SIZE, use_bias=False,\n",
    "              activation=activations.tanh))(context_embedded)\n",
    "\n",
    "    # AttentionLayer learns which path contexts are the most important.\n",
    "    # A code_vector $v$ now is the final representation for a piece of code.\n",
    "    code_vectors, attention_weights = AttentionLayer(name='attention')([context_after_dense, context_mask])\n",
    "\n",
    "    # $q(y) := softmax(v^T y)$\n",
    "    # Final dense layer: Learn how the method names should be represented.\n",
    "    # For each method name: The probability that a given code vector represents a method name is\n",
    "    # the dot product of those two values after softmax normalisation.\n",
    "    # The target_index is the key of the method name in the vocabulary with the highest probability.\n",
    "    target_index = Dense(len(vocab.method_names), use_bias=False,\n",
    "                         activation=activations.softmax, name='target_index')(\n",
    "        code_vectors)\n",
    "\n",
    "    inputs = [path_start_token, path_input, path_end_token, context_mask]\n",
    "    outputs = [target_index]\n",
    "    return keras.Model(name='code2vec', inputs=inputs, outputs=outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b27159fe",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Learns which of the contexts in the method are the most important.\n",
    "#\n",
    "# Adapted from: https://github.com/tech-srl/code2vec/blob/master/keras_attention_layer.py\n",
    "class AttentionLayer(Layer):\n",
    "    def __init__(self, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "\n",
    "    def build(self, inputs_shape):\n",
    "        shape_actual_input = inputs_shape[0]\n",
    "        self.input_length = int(shape_actual_input[1])\n",
    "        self.input_dim = int(shape_actual_input[2])\n",
    "\n",
    "        # The vector that defines how much each context should be weighted.\n",
    "        # Initialized with random values, model learns the actual ones.\n",
    "        attention_param_shape = (self.input_dim, 1)\n",
    "        self.attention_param = self.add_weight(name='attention_param',\n",
    "                                               shape=attention_param_shape,\n",
    "                                               initializer='uniform',\n",
    "                                               trainable=True, dtype=tf.float32)\n",
    "\n",
    "        super(AttentionLayer, self).build(shape_actual_input)\n",
    "\n",
    "    def call(self, inputs, **kwargs):\n",
    "        context = inputs[0]\n",
    "        mask = inputs[1]\n",
    "\n",
    "        # multiply each context with the attention to get the weight it should have in the final code_vector\n",
    "        attention_weights = kb.dot(context, self.attention_param)\n",
    "\n",
    "        if len(mask.shape) == 2:\n",
    "            mask = kb.expand_dims(mask, axis=2)\n",
    "        mask = kb.log(mask)\n",
    "        attention_weights += mask\n",
    "\n",
    "        # normalise weights\n",
    "        attention_weights = kb.softmax(attention_weights, axis=1)\n",
    "        # the code vector is just a weighted sum of contexts\n",
    "        code_vector = kb.sum(context * attention_weights, axis=1)\n",
    "\n",
    "        return code_vector, attention_weights\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return input_shape[0], input_shape[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a8ce849",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "model = build_code2vec_model(vocabulary)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c2fdf10",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model.compile(optimizer=optimizers.Adam(), loss=losses.CategoricalCrossentropy())\n",
    "\n",
    "# TODO: model.fit"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc30bb8c",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# ASTNN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ef7808f",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "Recent work provides the strong evidence that syntactic knowledge contributes more in modeling source code\n",
    "and can obtain better representation than traditional token-based methods. We will consider one example approach to creating code embeddings from syntactic information, ASTNN:\n",
    "\n",
    "Zhang, J., Wang, X., Zhang, H., Sun, H., Wang, K., & Liu, X. (2019, May). A novel neural source code representation based on abstract syntax tree. In 2019 IEEE/ACM 41st International Conference on Software Engineering (ICSE) (pp. 783-794). IEEE.\n",
    "\n",
    "ASTNN splits the large AST of one code fragment into a set of small trees at the statement level and performs tree-based neural embeddings on all statement trees. Recurrent Neural Networks (RNNs) are used to encode statements and the sequential dependency between the statements into a vector. These vectors capture the naturalness of source code, and can serve as a neural source code representation.\n",
    "\n",
    "\n",
    "As an example application for these embeddings, we can once again use code clone detection, which boils down to the following:\n",
    "\n",
    "- Compute vector embeddings $e_1$, $e_2 \\in \\mathbb{R}^m$ for two code snippets\n",
    "- The distance between the code snippets is $r = |e_1 - e_2| \\in \\mathbb{R}^m$\n",
    "- This can be reduced to a clone probability using a linear layer with sigmoid activation function $p = \\textrm{sigmoid}(r) \\in [0,1]$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4b390d9",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cc38311",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "The first step in producing the code embeddings consists of parsing the code, transforming the AST into a sequence of statement trees, and then replacing string labels of the tree nodes with numeric indices."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf82016d",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "We will eventually apply our encoding to a dataset of C programs (using the model trained by the authors of ASTNN), so in the following we will consider the syntax trees of C programs created by Python's C Parser library: [PyCParser](https://github.com/eliben/pycparser)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "344a1e94",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "code = \"\"\"\n",
    "int foo() {}\n",
    "\n",
    "int main(int argc, char** argv) {\n",
    "  if (argc > 0) {\n",
    "      foo();\n",
    "  }\n",
    "  return 0;\n",
    "}\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2711e07c",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "import pycparser\n",
    "code_parser = pycparser.c_parser.CParser()\n",
    "\n",
    "ast = code_parser.parse(code)\n",
    "ast"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "663a04f0",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "At the core of the ASTNN lies the extraction of statement trees from the AST. A statement tree is essentially a substree of the AST for a statement-node, and the list of statement trees is achieved by a preorder traversal of the AST."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59e40e0d",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "def get_statements(node):\n",
    "    name = node.__class__.__name__\n",
    "    print(f\"Current node: {name}\")\n",
    "    for _, child in node.children():\n",
    "        get_statements(child)\n",
    "    \n",
    "get_statements(ast)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4198cb50",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "For our example program, we would like to create statement trees for the if statement (`If`), the function call (`FuncCall`) , and the return statement (`Return`). ASTNN also treats function declarations (`FuncDef`) as special statement nodes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c6fa77b",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "def get_statements(node):\n",
    "    name = node.__class__.__name__\n",
    "    if name in [\"FuncDef\", \"FuncCall\", \"If\", \"Return\"]:\n",
    "        print(f\"Statement node: {name}\")\n",
    "    for _, child in node.children():\n",
    "        get_statements(child)\n",
    "    \n",
    "get_statements(ast)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81f07a80",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "A statement tree (ST-tree) rooted by the statement node $s \\in S$ is the tree consisting of node s and all of\n",
    "its descendants, but excluding any statements nodes. For example, for a method call statement, all child nodes are in the statement tree, since none of the children are statements. On the other hand, an if-statement consists of an expression as well as the conditionally executed statements:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe83c3ed",
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "code_parser.parse(\"int main() {if (42 > 0) { foo(); } }\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4321590",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "Consequently, when creating statement trees, if we encounter a `FuncDef`, `If`, `While`, `DoWhile`, or `Switch` statement, then we only include the first child in the statement tree, and ignore all other children (which are statements)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3d56961",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "A second exception are for-loops, which contain of multiple children: The `init`, `cond`, and `next` children are part of the for-statement itself, while the last child (`stmt`) is a statement and should be excluded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e24fbad",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "code_parser.parse(\"int main() {for (int i = 0; i < 10; i++) { foo(); } }\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1e21795",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "Note that the for-loop (and the other block-constructs we considered previously) define a single statement as child node, but of course it is common that they can contain more complex code blocks. In pycparser, these are are captured by `Compound` nodes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b9361a6",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "It is also worth noting that pycparser stores nodes as tuples `(str, node)`, so when traversing the AST to create statement trees we need to look at the second entry of such a tuple only. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "224fc110",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "class ASTNode(object):\n",
    "    def __init__(self, node):\n",
    "        self.node = node\n",
    "        self.name = self.node.__class__.__name__\n",
    "        children = node.children()\n",
    "        if self.name in ['FuncDef', 'If', 'While', 'DoWhile', 'Switch']:\n",
    "            self.__children = [ASTNode(children[0][1])]\n",
    "        elif self.name == 'For':\n",
    "            children = node.children()\n",
    "            self.__children = [ASTNode(children[c][1]) for c in range(0, len(children) - 1)]\n",
    "        else:\n",
    "            self.__children = [ASTNode(child) for _, child in node.children()]\n",
    "\n",
    "    def children(self):\n",
    "        return self.__children\n",
    "    \n",
    "    def __repr__(self):\n",
    "        return f\"{self.name}: {self.children()}\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d184758c",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "Now that we have a class to capture statement trees, we just need to implement the tree traversal to collect them for the statements we are interested (for the sake of the example for now only `FuncDef`, `FuncCall`, `If`, `Return`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b56518ca",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "def get_statement_trees(node):\n",
    "    name = node.__class__.__name__\n",
    "    trees = []\n",
    "    if name in [\"FuncDef\", \"FuncCall\", \"If\", \"Return\"]:\n",
    "        trees.append(ASTNode(node))\n",
    "\n",
    "    for _, child in node.children():\n",
    "        trees.extend(get_statement_trees(child))\n",
    "    \n",
    "    return trees\n",
    "    \n",
    "get_statement_trees(ast)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c051109",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "Our statement trees currently only describe the syntactic structure, but we have lost the lexical information about the actual tokens used (e.g., which methods were called). We add a `token` to our `ASTNode` class.\n",
    "- If the node is a leaf node (i.e., a variable name or a literal), then we use the actual lexeme.\n",
    "- For type declaration nodes we use the name of the type.\n",
    "- For operators, we use the operator symbol.\n",
    "- In other cases, we use the token type as the name."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ac3c22b",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "class ASTNode(ASTNode):\n",
    "    def __init__(self, node):\n",
    "        super().__init__(node)\n",
    "        self.token = self.get_token()\n",
    "    \n",
    "    def __repr__(self):\n",
    "        return f\"{self.token}: {self.children()}\"\n",
    "    \n",
    "    def is_leaf(self):\n",
    "        return len(self.node.children()) == 0    \n",
    "    \n",
    "    def get_token(self, lower=True):\n",
    "        name = self.node.__class__.__name__\n",
    "        token = name\n",
    "\n",
    "        if self.is_leaf():\n",
    "            attr_names = self.node.attr_names\n",
    "            if 'names' in attr_names:\n",
    "                token = self.node.names[0] # Identifiers\n",
    "            elif 'name' in attr_names:\n",
    "                token = self.node.name # ID\n",
    "            else:\n",
    "                token = self.node.value # Constant\n",
    "        else:\n",
    "            if name == 'TypeDecl':\n",
    "                token = self.node.declname\n",
    "            if self.node.attr_names:\n",
    "                attr_names = self.node.attr_names\n",
    "                if 'op' in attr_names:\n",
    "                    token = self.node.op # Binary Op\n",
    "\n",
    "        return token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2040ad1f",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "get_statement_trees(ast)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a2ef891",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "Since a neural network cannot process string labels of the trees, we first need to convert these to integers by taking their index in a vocabulary. In ASTNN this is done using a pre-trained word2vec vocabulary. If the label cannot be found, then an out-of-vocabulary index is assigned."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3023a0e7",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "The word2vec model is trained on the source code corpus; we will simply use the model generated by the ASTNN authors here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "caf869ac",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec\n",
    "w2v = Word2Vec.load(\"data/astnn/w2v_128\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7656eaa",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "The index of a word can be determined by directly looking the word up in the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a50adfba",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "def label_to_index(label: str) -> int:\n",
    "    return w2v.wv.get_index(label, default=len(w2v.wv))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c50910c",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "To convert the labels of a statement tree to numbers, we apply this to each node."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b20623f4",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "def tree_to_index(node: ASTNode):\n",
    "    token = node.token\n",
    "    indices = [label_to_index(token)]\n",
    "    for child in node.children():\n",
    "        indices.append(tree_to_index(child))\n",
    "    return indices"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd636fbc",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "Let's have a look at the statement trees for our example snippet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b7bac58",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "from typing import Any\n",
    "import pycparser\n",
    "\n",
    "for s in get_statement_trees(ast):\n",
    "    print(tree_to_index(s))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "180bf0cb",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "Our implementation of the AST traversal is limited to our example code snippet, and will not work on more general code snippets. To apply this to any C code snippets, let's use the full version, which mainly differs in which aspects of the AST it takes into consideration. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dc3c7a5",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "ast_block_token = ['FuncDef', 'If', 'While', 'DoWhile', 'Switch']\n",
    "\n",
    "\n",
    "class ASTNode(object):\n",
    "    def __init__(self, node, single=False):\n",
    "        self.node = node\n",
    "\n",
    "        self.__is_str = isinstance(self.node, str)\n",
    "        self.token = self.get_token()\n",
    "\n",
    "        if single:\n",
    "            self.__children = []\n",
    "        else:\n",
    "            self.__children = self.add_children()\n",
    "\n",
    "    def is_leaf(self):\n",
    "        if self.__is_str:\n",
    "            return True\n",
    "\n",
    "        return len(self.node.children()) == 0\n",
    "\n",
    "    def add_children(self):\n",
    "        if self.__is_str:\n",
    "            return []\n",
    "        children = self.node.children()\n",
    "        if self.token in ast_block_token:\n",
    "            return [ASTNode(children[0][1])]\n",
    "        elif self.token == 'For':\n",
    "            return [ASTNode(children[c][1]) for c in range(0, len(children) - 1)]\n",
    "        else:\n",
    "            return [ASTNode(child) for _, child in children]\n",
    "\n",
    "    def children(self):\n",
    "        return self.__children"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dff52b86",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "The retrieval of the right token is also slightly more involved beyond our snippet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e939c4fb",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "class ASTNode(ASTNode):\n",
    "    def get_token(self, lower=True):\n",
    "        if self.__is_str:\n",
    "            return self.node\n",
    "\n",
    "        name = self.node.__class__.__name__\n",
    "        token = name\n",
    "        is_name = False\n",
    "\n",
    "        if self.is_leaf():\n",
    "            attr_names = self.node.attr_names\n",
    "            if attr_names:\n",
    "                if 'names' in attr_names:\n",
    "                    token = self.node.names[0]\n",
    "                elif 'name' in attr_names:\n",
    "                    token = self.node.name\n",
    "                    is_name = True\n",
    "                else:\n",
    "                    token = self.node.value\n",
    "            else:\n",
    "                token = name\n",
    "        else:\n",
    "            if name == 'TypeDecl':\n",
    "                token = self.node.declname\n",
    "            if self.node.attr_names:\n",
    "                attr_names = self.node.attr_names\n",
    "                if 'op' in attr_names:\n",
    "                    if self.node.op[0] == 'p':\n",
    "                        token = self.node.op[1:]\n",
    "                    else:\n",
    "                        token = self.node.op\n",
    "        if token is None:\n",
    "            token = name\n",
    "        if lower and is_name:\n",
    "            token = token.lower()\n",
    "        return token\n",
    "    \n",
    "    def __repr__(self):\n",
    "        return f\"{self.get_token()}: {self.children()}\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db23722b",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "Finally, our retrieval of statement trees was slightly simplified. For example, the original ASTNN implementation also creates nodes for compound statements, and adds dedicated `End` nodes. These end-nodes do not match lexical tokens but inform the inference algorithm about the indentation of the code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97aa1905",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "def get_statements(node, statement_sequence):\n",
    "    children = node.children()\n",
    "    name = node.__class__.__name__\n",
    "    if name in ['FuncDef', 'If', 'For', 'While', 'DoWhile']:\n",
    "        statement_sequence.append(ASTNode(node))\n",
    "        if name != 'For':\n",
    "            inner_offset = 1\n",
    "        else:\n",
    "            inner_offset = len(children) - 1\n",
    "        for i in range(inner_offset, len(children)):\n",
    "            child = children[i][1]\n",
    "            if child.__class__.__name__ not in ['FuncDef', 'If', 'For', 'While', 'DoWhile', 'Compound']:\n",
    "                statement_sequence.append(ASTNode(child))\n",
    "            get_statements(child, statement_sequence)\n",
    "    elif name == 'Compound':\n",
    "        statement_sequence.append(ASTNode(name))\n",
    "        for _, child in children:\n",
    "            if child.__class__.__name__ not in ['If', 'For', 'While', 'DoWhile']:\n",
    "                statement_sequence.append(ASTNode(child))\n",
    "            get_statements(child, statement_sequence)\n",
    "        statement_sequence.append(ASTNode('End'))\n",
    "    else:\n",
    "        for _, child in children:\n",
    "            get_statements(child, statement_sequence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4d00d97",
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "statements = []\n",
    "get_statements(ast, statements)\n",
    "statements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a649293f",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "def to_statement_trees(ast) -> list[Any]:\n",
    "    statements = []\n",
    "    get_statements(ast, statements)\n",
    "    tree = []\n",
    "    for s in statements:\n",
    "        tree.append(tree_to_index(s))\n",
    "    return tree"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d5090bf",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "For the first assignment, this is essentially what you have to do as well. The output of this preprocessing now is fed to a model (which you don't have to implement for the assignment)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df4863f8",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39ed9a2c",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "Given the numeric encoding of tokens in the statement trees, the next step of building the embeddings consists of recursively creating vectors for statement trees."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9dba354",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "The lexical vector $v_n$ for a node $n$ is calculated as $v_n = W_e^T x_n$, where $x_n$ is the numerical representation of node $n$ (i.e., the index retrieved from Word2vec)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b08feac0",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "The vector representation of a node $n$ is computed by the following equation:\n",
    "\n",
    "$h = \\sigma (W_n^T + \\displaystyle\\sum_{i \\in [1, C]} h_i + b_n)$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "937cc627",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "Here, $W_n \\in \\mathcal{R}^{d \\times k}$ is the weight matrix with encoding dimensions $k$.  $b_n$ is a bias term, $h_i$ is the hidden state for each child $i$, $h$ is the updated hidden state, and $\\sigma$ is the activation function, which in ASTNN is the identity function."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "622f1fcc",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "The final encoding is then sampled with max pooling: $e_t = [max(h_{i1}, \\ldots, max(h_{ik})], i = 1, \\ldots, N$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4e6272a",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Dynamic Batching of trees (Algorithm 1, function `DynamicBatch()`)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ac7ee07",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "The computation of the vector encoding of a statement tree recursively depends on the vector encoding of its subnodes in the tree. Furthermore, For example, directly calculating $h$ for the two parents in one batch may also be impossible if the number of children of the two parents differs. This means that the computation of $h$ cannot be parallelised, but unfortunately that is exactly what is necessary in order to train a model on a large dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdb078f9",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "The authors of ASTNN have developed a batch processing algorithm that allows to encode multiple samples (i.e., code fragments) simultaneously. However, generally batch processing on multiway ST-trees makes it difficult since the number of children nodes varies for the parent nodes in the same position of one batch."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d3e1849",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "The algorithm batches $L$ samples of statement trees and then breadthfirst traverses them starting from the root nodes. For the current nodes $ns$ in the same position of the batch, the algorithm first calculates $v_n$ in batch, and then group all their children nodes by the node positions. That is, it groups all nodes at the same depth in the statement trees at the same position, such that the calculation of these nodes can be performed in parallel. Based on the grouping, the algorithm recursively performs batch processing on all children nodes. After getting the results of all children nodes, $h$ can be computed in batch, and all node vectors of calculated statement trees are recorded."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ad90feb",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "![Children Batching](figures/5_astnn_children_batching.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a10767e",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "Children batching\n",
    "\n",
    "- $ns$: `nodes`\n",
    "- $C$: `children`\n",
    "- $CI$: `children_index`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8326013d",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "Since our focus lies in the program analysis itself more than on the construction of an effective machine learning pipeline, we will present the ML-related code here, but will not go into as much detail as for the previous parts."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e25abc3",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "The following code sets up the batch tree encoder used by ASTNN, and initially sets up the required datastructures."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8fdb3e0",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "import numpy.typing\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from typing import Optional, Union\n",
    "\n",
    "\n",
    "class BatchTreeEncoder(tf.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        vocab_size: int,\n",
    "        vocab_embedding_dim: int,\n",
    "        encode_dim: int,\n",
    "        batch_size: int = 64,\n",
    "        pretrained_weight: Optional[numpy.typing.ArrayLike] = None,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.embedding = keras.layers.Embedding(vocab_size, vocab_embedding_dim)\n",
    "        self.embedding_dim = vocab_embedding_dim\n",
    "        self.encode_dim = encode_dim\n",
    "        self.W_c = keras.layers.Dense(encode_dim, input_shape=(vocab_embedding_dim,))\n",
    "        self.activation = tf.keras.activations.relu\n",
    "        self.batch_size = batch_size\n",
    "        self.node_list: list[tf.Tensor] = []\n",
    "        self.batch_node: Union[list[int], tf.Tensor] = []\n",
    "\n",
    "        # pretrained embedding from word2vec\n",
    "        if pretrained_weight is not None:\n",
    "            self.embedding.build((vocab_size, vocab_embedding_dim))\n",
    "            self.embedding.set_weights([pretrained_weight])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0d803bd",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "The actual traversal implements Algorithm 1 from the ASTNN paper. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2be3496c",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "class BatchTreeEncoder(BatchTreeEncoder):\n",
    "    def traverse(self, nodes, batch_index: list[int]) -> Optional[tf.Tensor]:\n",
    "        # Recursively compute embedding of multiple statement trees `nodes`\n",
    "        size = len(nodes)\n",
    "        if not size:\n",
    "            return None\n",
    "\n",
    "        # line 9: create an output placeholder `BC` for the batch input\n",
    "        batch_current = tf.zeros([size, self.embedding_dim], tf.float32)\n",
    "\n",
    "        index: list[int] = []\n",
    "        current_node: list[int] = []\n",
    "        children: list[list[int]] = []\n",
    "        children_index: list[list[int]] = []\n",
    "\n",
    "        for i, n in enumerate(nodes):\n",
    "            index.append(i)\n",
    "            current_node.append(n[0])\n",
    "\n",
    "            for j, child in enumerate(n[1:]):\n",
    "                # check if the child actually has a valid token index\n",
    "                if child[0] == -1:\n",
    "                    continue\n",
    "                # line 14: group children by their position\n",
    "                if len(children_index) <= j:\n",
    "                    children_index.append([i])\n",
    "                    children.append([child])\n",
    "                else:\n",
    "                    children_index[j].append(i)\n",
    "                    children[j].append(child)\n",
    "\n",
    "        index = tf.expand_dims(index, axis=-1)\n",
    "\n",
    "        batch_current = self._recurse(batch_current, batch_index, children, children_index, current_node, index, size, )\n",
    "        self._update_node_list(batch_current, batch_index)\n",
    "\n",
    "        return batch_current"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b665938",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "class BatchTreeEncoder(BatchTreeEncoder):\n",
    "    def _recurse(self, batch_current, batch_index, children, children_index, current_node, index, size, ):\n",
    "        # line 10: Equation 1\n",
    "        batch_current = self.W_c(\n",
    "            tf.tensor_scatter_nd_update(batch_current, index, self.embedding(tf.Variable(current_node)))\n",
    "        )\n",
    "\n",
    "        # line 17\n",
    "        for c_idx, child in enumerate(children):\n",
    "            # line 18: `\\tilde{h}`\n",
    "            zeros = tf.zeros([size, self.encode_dim], tf.float32)\n",
    "            batch_children_index = [batch_index[i] for i in children_index[c_idx]]\n",
    "\n",
    "            # line 19: n\n",
    "            # make a recursive call for each child to get the output of shape\n",
    "            # (1 x self.encode_dim)\n",
    "            tree = self.traverse(child, batch_children_index)\n",
    "            if tree is None:\n",
    "                continue\n",
    "\n",
    "            children_index_instance = tf.expand_dims(children_index[c_idx], axis=-1)\n",
    "            indices = tf.Variable(children_index_instance, tf.float32)\n",
    "            batch_current += tf.tensor_scatter_nd_update(zeros, indices, tree)\n",
    "\n",
    "        return batch_current"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4ec8fa1",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "class BatchTreeEncoder(BatchTreeEncoder):\n",
    "    def _update_node_list(self, batch_current, batch_index):\n",
    "        b_in = tf.Variable(batch_index)\n",
    "        b_in = tf.expand_dims(b_in, axis=-1)\n",
    "        self.node_list.append(tf.tensor_scatter_nd_update(self.batch_node, b_in, batch_current))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "116a921b",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "class BatchTreeEncoder(BatchTreeEncoder):\n",
    "    def __call__(self, inputs, batch_size):\n",
    "        self.batch_size = batch_size\n",
    "        self.node_list = []\n",
    "        self.batch_node = tf.zeros((self.batch_size, self.encode_dim), tf.float32)\n",
    "        self.traverse(inputs, list(range(self.batch_size)))\n",
    "        self.node_list = tf.stack(self.node_list)\n",
    "        return tf.reduce_max(self.node_list, axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59d61469",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Code clone detection with code embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1672338d",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "As described initially in this section, code clone detection can be implemented by calculating the vector embeddings for two code snippets $r_1$ and $r_2$, then measuring their distance $r = |r_1 - r_2$. A model is then trained for $\\hat{x} = W_o r + b_o$ with output $\\hat{y} = sigmoid(\\hat{x}) \\in [0, 1]$ such that the binary cross-entropy for a labelled dataset of code clones is minimized. The following code implements the model as described in the ASTNN paper."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46ead85c",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "from keras.layers import Lambda\n",
    "import tensorflow as tf\n",
    "from typing import Optional\n",
    "import numpy.typing\n",
    "\n",
    "\n",
    "class AstnnCloneDetection(tf.keras.Model):\n",
    "    def __init__(self, vocab_embedding_dim: int, hidden_dim: int, vocab_size: int, encode_dim: int, label_count: int, batch_size: int = 64, pretrained_weight: Optional[numpy.typing.NDArray] = None):\n",
    "        super().__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.batch_size = batch_size\n",
    "        self.embedding_dim = vocab_embedding_dim\n",
    "        self.encode_dim = encode_dim\n",
    "        self.label_count = label_count\n",
    "        self.encoder = BatchTreeEncoder(\n",
    "            vocab_size,\n",
    "            self.embedding_dim,\n",
    "            self.encode_dim,\n",
    "            self.batch_size,\n",
    "            pretrained_weight,\n",
    "        )\n",
    "        self.bigru = keras.layers.Bidirectional(keras.layers.GRU(self.hidden_dim, return_sequences=True))\n",
    "        self.hidden_state: list[tf.Tensor] = []\n",
    "\n",
    "        self.l1_layer = Lambda(lambda tensors: tf.abs(tensors[0] - tensors[1]))\n",
    "        self.output_layer = keras.layers.Dense(self.label_count, input_shape=(self.hidden_dim * 2,), activation=keras.activations.sigmoid)\n",
    "\n",
    "        self._reset_RNN_hidden_state()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fa8714a",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "class AstnnCloneDetection(AstnnCloneDetection):\n",
    "    def _reset_RNN_hidden_state(self) -> None:\n",
    "        self.hidden_state = [tf.zeros([self.batch_size, self.hidden_dim]) for _ in range(2)]\n",
    "\n",
    "    def _setup_for_next_batch(self, batch_size: int) -> None:\n",
    "        self.batch_size = batch_size\n",
    "        self._reset_RNN_hidden_state()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf3a3e2a",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Representation of a Statement Sequence\n",
    "\n",
    "- $T$ statements of a code snippet given in $x$\n",
    "- each statement is encoded using the `BatchTreeEncoder` and placed on the `result_stack`\n",
    "- Gated Recurrent Unit (GRU) in both directions over encoded statements $e_i$ to learn about relation to statements before and after in the code\n",
    "\n",
    "    - $\\overrightarrow{h_t} = \\overrightarrow{\\text{GRU}}(e_t)$\n",
    "    - $\\overleftarrow{h_t} = \\overleftarrow{\\text{GRU}}(e_t)$\n",
    "    - $h_t := \\overleftarrow{h_t}, \\overrightarrow{h_t}$\n",
    "\n",
    "- reduce vectors to most important features by max pooling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8b86f97",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "class AstnnCloneDetection(AstnnCloneDetection):\n",
    "    def encode(self, x: tf.Tensor):\n",
    "        lengths = [len(item) for item in x]\n",
    "\n",
    "        # statement trees to encode\n",
    "        encodes = [statement_tree for code in x for statement_tree in code]\n",
    "\n",
    "        # line 4: pass the statement trees to the batch tree encoder\n",
    "        encoded = self.encoder(encodes, sum(lengths))\n",
    "        # line 24: collect onto S\n",
    "        result_stack = self._collect_stack(lengths, encoded)\n",
    "\n",
    "        # line 5: get BV\n",
    "        gru_out = self.bigru(result_stack, self.hidden_state)\n",
    "        gru_out = tf.transpose(gru_out, perm=[0, 2, 1])\n",
    "        gru_out = tf.reduce_max(gru_out, axis=[2], keepdims=True)\n",
    "        gru_out = tf.squeeze(gru_out, 2)\n",
    "\n",
    "        return gru_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4978199",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "class AstnnCloneDetection(AstnnCloneDetection):\n",
    "    def _collect_stack(self, lengths: list[int], encoded: tf.Tensor) -> tf.Tensor:\n",
    "        max_length = max(lengths)\n",
    "        result_stack_tmp: list[tf.Tensor] = []\n",
    "        start: int = 0\n",
    "        end: int = 0\n",
    "        for length in lengths:\n",
    "            end += length\n",
    "            if max_length - length:\n",
    "                filler = tf.zeros((max_length - length, self.encode_dim))\n",
    "                result_stack_tmp.append(filler)\n",
    "            result_stack_tmp.append(encoded[start:end])\n",
    "            start = end\n",
    "\n",
    "        # reshape the stack S to be usable as input for the GRU\n",
    "        result_stack = tf.concat(result_stack_tmp, axis=0)\n",
    "        return tf.reshape(result_stack, [self.batch_size, max_length, -1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa5eb16f",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "class AstnnCloneDetection(AstnnCloneDetection):\n",
    "    def call(self, inputs, training=None, mask=None, **kwargs):\n",
    "        code1, code2 = inputs\n",
    "        self._setup_for_next_batch(batch_size=1)\n",
    "        vec1, vec2 = self.encode(code1), self.encode(code2)\n",
    "        return self.output_layer(self.l1_layer([vec1, vec2]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9830e8ad",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Making a Prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "624c6e80",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "To see the model in action, of course we need to train it on a large dataset. A labelled dataset of code clones is available  in the [BigCloneBench](https://github.com/clonebench/BigCloneBench) dataset. ASTNN was trained on this dataset, and we will simply load the vocabulary and model trained by the authors of ASTNN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b28a0304",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec\n",
    "\n",
    "w2v = Word2Vec.load(\"data/astnn/w2v_128\")\n",
    "\n",
    "def load_model() -> AstnnCloneDetection:\n",
    "    vocab_size = len(w2v.wv.vectors) + 1\n",
    "    w2v_embeddings = numpy.zeros((vocab_size, w2v.vector_size), dtype=float)\n",
    "    w2v_embeddings[: w2v.wv.vectors.shape[0]] = w2v.wv.vectors\n",
    "    model = AstnnCloneDetection(vocab_embedding_dim=128, hidden_dim=100, vocab_size=vocab_size, encode_dim=128, label_count=1, batch_size=1, pretrained_weight=w2v_embeddings)\n",
    "    dummy = [[[33, [2, [30, [40, [4]]]]]]]\n",
    "    x = model((dummy, dummy)) # Tensorflow lazy init: force initialisation using dummy data\n",
    "    model.load_weights(\"./data/astnn/weights/weights\")\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d594f3e9",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "model = load_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd259214",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "With this model, we can use any new pair of C code snippets and query the predicted label (0 = no clones, 1 = clones)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "542c6bff",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "def predict(model: AstnnCloneDetection, code1: str, code2: str) -> float:\n",
    "    code_parser = pycparser.c_parser.CParser()\n",
    "    c1 = code_parser.parse(code1)\n",
    "    c2 = code_parser.parse(code2)\n",
    "\n",
    "    code1, code2 = to_statement_trees(c1), to_statement_trees(c2)\n",
    "    output = model(([code1], [code2]))\n",
    "    return output[-1][-1].numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "946476db",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "Let's define some usual example code snippets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a7bbae1",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "code1 = \"\"\"\n",
    "int foo(int x) {\n",
    "  if (x > 0) {\n",
    "      printf(\"Hallo\");\n",
    "  } else {\n",
    "      printf(\"Nicht hallo\");  \n",
    "  }\n",
    "  return 0;\n",
    "}\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3334b4f6",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "code2 = \"\"\"\n",
    "int bar(int x) {\n",
    "  if (x > 0) {\n",
    "      printf(\"Hallo\");\n",
    "  } else {\n",
    "      printf(\"Nicht hallo\");  \n",
    "  }\n",
    "  return 0;\n",
    "}\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f0a4114",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "code3 = \"\"\"\n",
    "int bar(int x) {\n",
    "  printf(\"Not a clone\");\n",
    "  return 0;\n",
    "}\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77317d3e",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "For example, the first and second code snippet are identical except for the function name, so we would expect it to be detected as a clone pair:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9a6bc08",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "predict(model, code1, code2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ba5683f",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "In contrast, the first and third snippet represent entirely different code and should thus not be detected as a clone pair."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89b20841",
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "predict(model, code1, code3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19fc1493",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "The prediction is turned into a label by using a threshold $\\delta$, such that the two code snippets are a clone pair if the prediction $p > \\delta$. For example, the ASTNN experiments set $\\delta = 0.5$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f178ce25",
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "predict(model, code2, code3)"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
